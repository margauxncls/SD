{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149938de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773c1396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2076740bd60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, test_data = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "x_train = train_data[0].reshape(-1,784).astype(float) / 255\n",
    "y_train = train_data[1]\n",
    "print(y_train[0])\n",
    "plt.imshow(train_data[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4b521b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " digits (InputLayer)         [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                25120     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,506\n",
      "Trainable params: 26,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# source: https://keras.io/guides/training_with_built_in_methods/\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(32, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(32, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, activation=None, name=\"predictions\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#def loss(true, pred):\n",
    "#    val = 0.\n",
    "#    for i in range(len(true)):\n",
    "#        val -= np.log(pred[i, true[i]])\n",
    "#    return val / len(true)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b18b6bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 1, 1, 5, 9, 2, 2, 7, 1, 7, 5, 2, 1, 1, 1, 1, 5, 1, 5,\n",
       "       1, 1, 2, 8, 1, 9, 2, 2, 1, 1, 2, 1, 0, 2, 1, 7, 9, 2, 2, 2, 4, 2,\n",
       "       5, 2, 1, 2, 2, 3, 5, 9, 9, 2, 9, 2, 5, 1, 1, 3, 2, 2, 1, 2, 7, 1,\n",
       "       1, 2, 2, 0, 2, 1, 2, 1, 7, 2, 3, 3, 2, 1, 1, 5, 2, 2, 9, 1, 5, 5,\n",
       "       4, 2, 1, 6, 1, 1, 2, 2, 2, 1, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# untrained model\n",
    "logits = model(x_train)\n",
    "np.argmax(logits, 1)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "35d74f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0,\n",
       "       9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9,\n",
       "       3, 9, 8, 5, 9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5,\n",
       "       6, 1, 0, 0, 1, 7, 1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9,\n",
       "       0, 4, 6, 7, 4, 6, 8, 0, 7, 8, 3, 1], dtype=uint8)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what we would like to get\n",
    "y_train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4c0073e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2871685"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Computing a stochastic gradient\n",
    "\n",
    "# Source: https://keras.io/guides/writing_a_training_loop_from_scratch/\n",
    "\n",
    "# Example of gradient computation using automatic differentiation\n",
    "\n",
    "x_train_batch = x_train[:8]\n",
    "y_train_batch = y_train[:8]\n",
    "with tf.GradientTape() as tape:\n",
    "        # Run the forward pass of the layer.\n",
    "        # The operations that the layer applies\n",
    "        # to its inputs are going to be recorded\n",
    "        # on the GradientTape.\n",
    "        logits = model(x_train_batch)  # Logits for this minibatch\n",
    "\n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value = loss(y_train_batch, logits)\n",
    "\n",
    "# Use the gradient tape to automatically retrieve\n",
    "# the gradients of the trainable variables with respect to the loss.\n",
    "grad = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "## Updating weights\n",
    "\n",
    "def update_weights(model, update):\n",
    "    new_weights = model.trainable_weights.copy()\n",
    "    for i in range(len(new_weights)):\n",
    "        new_weights[i] = new_weights[i] + update[i]\n",
    "    model.set_weights(new_weights)\n",
    "\n",
    "\n",
    "## Evaluate model\n",
    "samples_for_evaluation = np.random.randint(0, 10000, 1000)\n",
    "logits = model(x_train[samples_for_evaluation])\n",
    "loss(y_train[samples_for_evaluation], logits).numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b115366",
   "metadata": {},
   "source": [
    "#### Question 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b9682",
   "metadata": {},
   "source": [
    "#### We need 26 506 optimization variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d7086",
   "metadata": {},
   "source": [
    "### (5) Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5692ed8",
   "metadata": {},
   "source": [
    "#### Question 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c5a9ead7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient algorithm\n",
    "\n",
    "def train_neural_network(x_train, y_train, model, learning_rate, num_epochs):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, x in enumerate(x_train):\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(np.array([x]))\n",
    "                val_loss = loss(y_train[i],y_pred)\n",
    "            grads = tape.gradient(val_loss, model.trainable_variables)\n",
    "            losses.append(val_loss)\n",
    "            \n",
    "            # Update model parameters using SGD\n",
    "            update_weights(model, [-learning_rate*grad for grad in grads])\n",
    "            \"\"\"params, n = model.trainable_variables.copy(), 0\n",
    "            for param, grad in zip(model.trainable_variables, grads):\n",
    "                params[n] = param - learning_rate * grad\n",
    "                n+=1\n",
    "            model.set_weights(params)\"\"\"\n",
    "            \n",
    "            if i%1000 == 0:\n",
    "                print(i, val_loss)\n",
    "                \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb92687",
   "metadata": {},
   "source": [
    "#### Question 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1ed21e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tf.Tensor(2.2458735, shape=(), dtype=float32)\n",
      "1000 tf.Tensor(0.22194426, shape=(), dtype=float32)\n",
      "2000 tf.Tensor(1.8339624, shape=(), dtype=float32)\n",
      "3000 tf.Tensor(1.3936731, shape=(), dtype=float32)\n",
      "4000 tf.Tensor(0.011988493, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [161]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the algorithm for one epoch and with a learning rate that satisfies the conditions required by theory\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the objective value as a function of the iterations\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses)\n",
      "Input \u001b[1;32mIn [160]\u001b[0m, in \u001b[0;36mtrain_neural_network\u001b[1;34m(x_train, y_train, model, learning_rate, num_epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x_train):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m----> 8\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m loss(y_train[i],y_pred)\n\u001b[0;32m     10\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(val_loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    488\u001b[0m   layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[1;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1014\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1017\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:458\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    441\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m  In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 458\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:596\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    593\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 596\u001b[0m outputs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mlayer(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_id, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39mflat_output_ids, tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1014\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1017\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\dense.py:232\u001b[0m, in \u001b[0;36mDense.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    229\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mset_shape(output_shape)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m--> 232\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3523\u001b[0m, in \u001b[0;36mbias_add\u001b[1;34m(value, bias, data_format, name)\u001b[0m\n\u001b[0;32m   3520\u001b[0m   value \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(value, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3521\u001b[0m   bias \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(bias, dtype\u001b[38;5;241m=\u001b[39mvalue\u001b[38;5;241m.\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_nn_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:673\u001b[0m, in \u001b[0;36mbias_add\u001b[1;34m(value, bias, data_format, name)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    672\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 673\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBiasAdd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    676\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the algorithm for one epoch and with a learning rate that satisfies the conditions required by theory\n",
    "losses = train_neural_network(x_train, y_train, model, 0.01, 1)\n",
    "\n",
    "# Plot the objective value as a function of the iterations\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Objective Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e45193",
   "metadata": {},
   "source": [
    "### (6) Empirical risk minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc4c900",
   "metadata": {},
   "source": [
    "#### Question 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "16641e62",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [131]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss(y_train[i], y_pred)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate the gradients with respect to the parameters\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss_value, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Update the parameters using SGD\u001b[39;00m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the optimization algorithm\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Number of epochs to run\n",
    "num_epochs = 1\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop over each sample in the training data\n",
    "    for i, x in enumerate(x_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Make a prediction for a single sample\n",
    "            y_pred = model(np.array([x]))\n",
    "            \n",
    "            # Calculate the loss for this sample\n",
    "            loss_value = loss(y_train[i], y_pred)\n",
    "        \n",
    "        # Calculate the gradients with respect to the parameters\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        \n",
    "        # Update the parameters using SGD\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "    print(f\"Epoch {epoch}: Loss = {loss_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a253f",
   "metadata": {},
   "source": [
    "### Question 6 : Empirical risk minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba54737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.00112255965359509\n",
      "Epoch 1: Loss = 0.0015922023449093103\n",
      "Epoch 2: Loss = 0.0004503904783632606\n",
      "Epoch 3: Loss = 0.00019905969384126365\n",
      "Epoch 4: Loss = 8.761498611420393e-05\n",
      "Epoch 5: Loss = 3.659658250398934e-05\n",
      "Epoch 6: Loss = 5.245071224635467e-05\n",
      "Epoch 7: Loss = 0.0016662298003211617\n",
      "Epoch 8: Loss = 0.00043501926120370626\n",
      "Epoch 9: Loss = 2.264974000354414e-06\n"
     ]
    }
   ],
   "source": [
    "# Define the optimization algorithm\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Number of epochs to run\n",
    "num_epochs = 10\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop over each sample in the training data\n",
    "    for i, x in enumerate(x_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Make a prediction for a single sample\n",
    "            y_pred = model(np.array([x]))\n",
    "            \n",
    "            # Calculate the loss for this sample\n",
    "            loss_value = loss(y_train[i], y_pred)\n",
    "        \n",
    "        # Calculate the gradients with respect to the parameters\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        \n",
    "        # Update the parameters using SGD\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "    print(f\"Epoch {epoch}: Loss = {loss_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff26246",
   "metadata": {},
   "source": [
    "Super long Ã  tourner..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd9ff0",
   "metadata": {},
   "source": [
    "#### Question 6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583cd3b",
   "metadata": {},
   "source": [
    "Using a batch of samples in gradient descent algorithms, such as batch gradient descent or mini-batch gradient descent, has several advantages over using a single sample (stochastic gradient descent):\n",
    "\n",
    "    (1) Faster convergence: Using a batch of samples to compute the gradients provides a more accurate estimate of the gradient, leading to faster convergence and better optimization results.\n",
    "\n",
    "    (2) Better use of hardware resources: Processing a batch of samples at once can be more computationally efficient than processing a single sample at a time, as it can take advantage of parallel processing and vectorization hardware.\n",
    "\n",
    "    (3) Regularization: Using a batch of samples instead of a single sample can help reduce overfitting, as the model's parameters are updated based on the average gradient over the entire batch. This provides a form of regularization that can help prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e3a8a",
   "metadata": {},
   "source": [
    "#### Question 6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24d39aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.18711067736148834\n",
      "Epoch 1: Loss = 0.13649453222751617\n",
      "Epoch 2: Loss = 0.10861142724752426\n",
      "Epoch 3: Loss = 0.09280143678188324\n",
      "Epoch 4: Loss = 0.08049016445875168\n",
      "Epoch 5: Loss = 0.0690719410777092\n",
      "Epoch 6: Loss = 0.06171197444200516\n",
      "Epoch 7: Loss = 0.05560793727636337\n",
      "Epoch 8: Loss = 0.050929367542266846\n",
      "Epoch 9: Loss = 0.04819677397608757\n"
     ]
    }
   ],
   "source": [
    "# Define the optimization algorithm\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Number of epochs to run\n",
    "num_epochs = 10\n",
    "\n",
    "# Batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop over each batch in the training data\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train[i:i + batch_size]\n",
    "        y_batch = y_train[i:i + batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Make predictions for a batch of samples\n",
    "            y_pred = model(np.array(x_batch))\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            loss_value = loss(y_batch, y_pred)\n",
    "        \n",
    "        # Calculate the gradients with respect to the parameters\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        \n",
    "        # Update the parameters using SGD\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "    print(f\"Epoch {epoch}: Loss = {loss_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf71280",
   "metadata": {},
   "source": [
    "### (7) Evaluation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4d8cc",
   "metadata": {},
   "source": [
    "#### Question 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f10b2a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17605954"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate model\n",
    "samples_for_evaluation = np.random.randint(0, 10000, 1000)\n",
    "logits = model(x_train[samples_for_evaluation])\n",
    "loss(y_train[samples_for_evaluation], logits).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d184137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate model\n",
    "samples_for_evaluation = np.random.randint(0, 10000, 1000)\n",
    "logits = model(x_train[samples_for_evaluation])\n",
    "loss(y_train[samples_for_evaluation], logits).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
